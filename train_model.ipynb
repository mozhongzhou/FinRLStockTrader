{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install finrl library\n",
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查GPU可用性...\n",
      "✓ 发现 1 个可用的GPU设备\n",
      "✓ 当前使用: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号\n",
    "import torch\n",
    "import time\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "print(\"检查GPU可用性...\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    cuda_device_count = torch.cuda.device_count()\n",
    "    cuda_device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"✓ 发现 {cuda_device_count} 个可用的GPU设备\")\n",
    "    print(f\"✓ 当前使用: {cuda_device_name}\")\n",
    "else:\n",
    "    print(\"✗ 未发现可用的GPU，将使用CPU进行训练\")\n",
    "\n",
    "# 确保模型保存目录存在\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])\n",
    "# 设置随机种子以确保结果可复现\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载训练数据: 950608 条记录\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>A</td>\n",
       "      <td>37.353016</td>\n",
       "      <td>41.310001</td>\n",
       "      <td>40.369999</td>\n",
       "      <td>41.180000</td>\n",
       "      <td>1529200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.992889</td>\n",
       "      <td>36.013226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37.353016</td>\n",
       "      <td>37.353016</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>24.320433</td>\n",
       "      <td>27.860001</td>\n",
       "      <td>26.837500</td>\n",
       "      <td>27.847500</td>\n",
       "      <td>212818400.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.992889</td>\n",
       "      <td>36.013226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>24.320433</td>\n",
       "      <td>24.320433</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>ABBV</td>\n",
       "      <td>43.156208</td>\n",
       "      <td>66.400002</td>\n",
       "      <td>65.440002</td>\n",
       "      <td>65.440002</td>\n",
       "      <td>5086100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.992889</td>\n",
       "      <td>36.013226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>43.156208</td>\n",
       "      <td>43.156208</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>ABT</td>\n",
       "      <td>36.915035</td>\n",
       "      <td>45.450001</td>\n",
       "      <td>44.639999</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>3216600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.992889</td>\n",
       "      <td>36.013226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.915035</td>\n",
       "      <td>36.915035</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>ACGL</td>\n",
       "      <td>18.539352</td>\n",
       "      <td>19.860001</td>\n",
       "      <td>19.426666</td>\n",
       "      <td>19.733334</td>\n",
       "      <td>1101600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.992889</td>\n",
       "      <td>36.013226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.539352</td>\n",
       "      <td>18.539352</td>\n",
       "      <td>17.790001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   tic      close       high        low       open       volume  \\\n",
       "                                                                               \n",
       "0  2015-01-02     A  37.353016  41.310001  40.369999  41.180000    1529200.0   \n",
       "0  2015-01-02  AAPL  24.320433  27.860001  26.837500  27.847500  212818400.0   \n",
       "0  2015-01-02  ABBV  43.156208  66.400002  65.440002  65.440002    5086100.0   \n",
       "0  2015-01-02   ABT  36.915035  45.450001  44.639999  45.250000    3216600.0   \n",
       "0  2015-01-02  ACGL  18.539352  19.860001  19.426666  19.733334    1101600.0   \n",
       "\n",
       "   day  macd    boll_ub    boll_lb  rsi_30     cci_30  dx_30  close_30_sma  \\\n",
       "                                                                             \n",
       "0  4.0   0.0  37.992889  36.013226     0.0 -66.666667  100.0     37.353016   \n",
       "0  4.0   0.0  37.992889  36.013226     0.0 -66.666667  100.0     24.320433   \n",
       "0  4.0   0.0  37.992889  36.013226     0.0 -66.666667  100.0     43.156208   \n",
       "0  4.0   0.0  37.992889  36.013226     0.0 -66.666667  100.0     36.915035   \n",
       "0  4.0   0.0  37.992889  36.013226     0.0 -66.666667  100.0     18.539352   \n",
       "\n",
       "   close_60_sma        vix  turbulence  \n",
       "                                        \n",
       "0     37.353016  17.790001         0.0  \n",
       "0     24.320433  17.790001         0.0  \n",
       "0     43.156208  17.790001         0.0  \n",
       "0     36.915035  17.790001         0.0  \n",
       "0     18.539352  17.790001         0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载预处理后的训练数据\n",
    "processed_data_file = \"data/processed_data/train_data_20150101~20250101.csv\"\n",
    "\n",
    "# 检查文件是否存在\n",
    "if not os.path.exists(processed_data_file):\n",
    "    raise FileNotFoundError(\n",
    "        f\"找不到处理后的数据文件: {processed_data_file}，请先运行 process_data.ipynb\"\n",
    "    )\n",
    "\n",
    "# 加载训练数据\n",
    "train = pd.read_csv(processed_data_file)\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = [\"\"]\n",
    "\n",
    "print(f\"加载训练数据: {len(train)} 条记录\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建交易环节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "股票数量: 472, 状态空间维度: 4721\n",
      "环境类型: <class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "# 构建交易环境的参数\n",
    "# 计算环境参数\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
    "print(f\"股票数量: {stock_dimension}, 状态空间维度: {state_space}\")\n",
    "\n",
    "# 设置环境参数\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension  # 交易成本 0.1%\n",
    "num_stock_shares = [0] * stock_dimension  # 初始持有股票数量\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,  # 每个交易周期的最大交易次数\n",
    "    \"initial_amount\": 1000000,  # 初始资金\n",
    "    \"num_stock_shares\": num_stock_shares, # 初始持有股票数量\n",
    "    \"buy_cost_pct\": buy_cost_list,# 买入成本\n",
    "    \"sell_cost_pct\": sell_cost_list, # 卖出成本\n",
    "    \"state_space\": state_space, # 状态空间\n",
    "    \"stock_dim\": stock_dimension, # 股票数量\n",
    "    \"tech_indicator_list\": INDICATORS, # 技术指标列表\n",
    "    \"action_space\": stock_dimension,# 动作空间\n",
    "    \"reward_scaling\": 1e-4,# 奖励缩放\n",
    "}\n",
    "\n",
    "# 构建交易环境\n",
    "e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(f\"环境类型: {type(env_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU设备: cpu\n",
      "GPU设备: cuda\n",
      "选中的算法及其训练设备:\n",
      "A2C: ✓ (设备: CPU)\n",
      "DDPG: ✓ (设备: GPU)\n",
      "PPO: ✓ (设备: GPU)\n",
      "TD3: ✓ (设备: GPU)\n",
      "SAC: ✓ (设备: GPU)\n"
     ]
    }
   ],
   "source": [
    "# 算法选择与GPU/CPU设置\n",
    "# 设置为 True 选择使用相应算法\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True\n",
    "\n",
    "# GPU相关设置\n",
    "if use_cuda:\n",
    "    # 根据算法特性分配设备\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    gpu_device = torch.device(\"cuda\")\n",
    "\n",
    "    print(f\"CPU设备: {cpu_device}\")\n",
    "    print(f\"GPU设备: {gpu_device}\")\n",
    "else:\n",
    "    # 如果没有GPU，所有模型都使用CPU\n",
    "    cpu_device = gpu_device = torch.device(\"cpu\")\n",
    "    print(\"未检测到GPU，所有模型将使用CPU\")\n",
    "\n",
    "# 因为有GPU加速，可以适当增加训练步数提高性能\n",
    "a2c_timesteps = 50000  # 即使有GPU也用较少步数，因为在CPU上运行\n",
    "ddpg_timesteps = 100000 if use_cuda else 50000\n",
    "ppo_timesteps = 100000 if use_cuda else 50000\n",
    "td3_timesteps = 80000 if use_cuda else 30000\n",
    "sac_timesteps = 80000 if use_cuda else 30000\n",
    "\n",
    "print(\"选中的算法及其训练设备:\")\n",
    "print(f\"A2C: {'✓' if if_using_a2c else '✗'} (设备: CPU)\")\n",
    "print(f\"DDPG: {'✓' if if_using_ddpg else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")\n",
    "print(f\"PPO: {'✓' if if_using_ppo else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")\n",
    "print(f\"TD3: {'✓' if if_using_td3 else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")\n",
    "print(f\"SAC: {'✓' if if_using_sac else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== 开始训练 A2C 模型 ========\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007, 'device': device(type='cpu')}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n",
      "开始训练，总步数: 50000\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -670       |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -425       |\n",
      "|    reward             | 0.34803268 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -671       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.42e+03  |\n",
      "|    reward             | -2.2179449 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 7.27       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -671       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -316       |\n",
      "|    reward             | -1.7063044 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.42       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 156        |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -671       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 2.49e+03   |\n",
      "|    reward             | 0.20818733 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 54.3       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 12       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 195      |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -671     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 376      |\n",
      "|    reward             | 0.675648 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.637    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 233       |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -672      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -2.58e+03 |\n",
      "|    reward             | 2.4125783 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 24.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 273        |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -672       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -3.56e+03  |\n",
      "|    reward             | -2.0161269 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 29.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 12          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 313         |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -672        |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 3.15e+03    |\n",
      "|    reward             | -0.22869758 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 67.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 352       |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -673      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 1.63e+03  |\n",
      "|    reward             | 1.9412365 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.97      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 392        |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -674       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 2.66e+03   |\n",
      "|    reward             | -1.2943757 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 23.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 435       |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -674      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -5.39e+03 |\n",
      "|    reward             | 7.340446  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 72.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 12       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 477      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -674     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 4.48e+03 |\n",
      "|    reward             | 3.303623 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 71.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 520        |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -674       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -106       |\n",
      "|    reward             | 0.47122994 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.174      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 562        |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -675       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -2.03e+03  |\n",
      "|    reward             | -0.8057866 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 19.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 605       |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -675      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 4.15e+03  |\n",
      "|    reward             | 1.6522622 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 47.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 12          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 648         |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -675        |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 9.37e+03    |\n",
      "|    reward             | -0.11883441 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 244         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 691        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -675       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -537       |\n",
      "|    reward             | 0.19644526 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.894      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 734       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -676      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 261       |\n",
      "|    reward             | 1.3028504 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.96      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 776       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -676      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -3.31e+03 |\n",
      "|    reward             | 5.2427955 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 25.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 817       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -676      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -5.72e+03 |\n",
      "|    reward             | -6.060325 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 125       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 859        |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -676       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | -1.22e+03  |\n",
      "|    reward             | 0.07888565 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 902        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -677       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | -1.4e+03   |\n",
      "|    reward             | 0.37241507 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.34       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 944       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -677      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 812       |\n",
      "|    reward             | -4.312554 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.21      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 987       |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -678      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -392      |\n",
      "|    reward             | 1.1589574 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 11        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 1029       |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -678       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | 457        |\n",
      "|    reward             | 0.17747034 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.01       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 12         |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 1072       |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -679       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | -1.45e+03  |\n",
      "|    reward             | -1.8339854 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 7.26       |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 12            |\n",
      "|    iterations         | 2700          |\n",
      "|    time_elapsed       | 1115          |\n",
      "|    total_timesteps    | 13500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -679          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2699          |\n",
      "|    policy_loss        | 69.9          |\n",
      "|    reward             | 1.2551375e-05 |\n",
      "|    std                | 1.02          |\n",
      "|    value_loss         | 0.176         |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 1158      |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -679      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -217      |\n",
      "|    reward             | 4.398934  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.63      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 12           |\n",
      "|    iterations         | 2900         |\n",
      "|    time_elapsed       | 1201         |\n",
      "|    total_timesteps    | 14500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -680         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2899         |\n",
      "|    policy_loss        | -861         |\n",
      "|    reward             | -0.013988273 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 1.79         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 1243      |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -680      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | -354      |\n",
      "|    reward             | 0.7491597 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.961     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 1285      |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -681      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 2.14e+03  |\n",
      "|    reward             | 2.6606786 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 13        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 1325      |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -681      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -1.77e+03 |\n",
      "|    reward             | 0.7927321 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.77      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 12          |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 1365        |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -681        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | 1.24e+03    |\n",
      "|    reward             | -0.27720132 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 3.92        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 1409      |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -682      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 1.29e+03  |\n",
      "|    reward             | 0.2321844 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.9       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 1452      |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -682      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 1.43e+03  |\n",
      "|    reward             | 3.0191836 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 14.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 1495      |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -682      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 7.46e+03  |\n",
      "|    reward             | 3.8378966 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 170       |\n",
      "-------------------------------------\n",
      "day: 2013, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2783258.42\n",
      "total_reward: 1783258.42\n",
      "total_cost: 22391.46\n",
      "total_trades: 480867\n",
      "Sharpe: 0.617\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 12        |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 1539      |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -683      |\n",
      "|    explained_variance | -0.00824  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 823       |\n",
      "|    reward             | 1.8574266 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.74      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 1583      |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -683      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -902      |\n",
      "|    reward             | -2.391155 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.32      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 1626      |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -683      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -6.8e+03  |\n",
      "|    reward             | 4.3825526 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 115       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 1670       |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -683       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | 1.08e+03   |\n",
      "|    reward             | -16.909063 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 44.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 1714       |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -684       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | 2.6        |\n",
      "|    reward             | 0.35912126 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.0442     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 1758      |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -684      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | 641       |\n",
      "|    reward             | 1.4956084 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.95      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 1801      |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -685      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | 5.77e+03  |\n",
      "|    reward             | 3.7768834 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 128       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 11       |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 1845     |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -685     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 5.48e+03 |\n",
      "|    reward             | 8.414431 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 99.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 1889       |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -685       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 139        |\n",
      "|    reward             | 0.38944703 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.642      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 1933       |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -686       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 2.79e+03   |\n",
      "|    reward             | -2.4797843 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 19.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 1976      |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -686      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -1.23e+03 |\n",
      "|    reward             | 2.969599  |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 35.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 2021      |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -686      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -1.57e+04 |\n",
      "|    reward             | 0.1124023 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 608       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 2065       |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -687       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | -973       |\n",
      "|    reward             | -1.9635248 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.06       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 2108       |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -687       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 2.74e+03   |\n",
      "|    reward             | -3.3266084 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 16.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 2152       |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -687       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 6.92e+03   |\n",
      "|    reward             | -3.7420495 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 197        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 2196      |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -688      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -3.28e+03 |\n",
      "|    reward             | -4.837272 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 148       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 2239       |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -688       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | 465        |\n",
      "|    reward             | 0.37453267 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.951      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 2283      |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -689      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | -3.52e+03 |\n",
      "|    reward             | 1.7690927 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 39.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 2326      |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -689      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -1.01e+04 |\n",
      "|    reward             | 5.636046  |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 242       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 2370      |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -689      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 8.44e+03  |\n",
      "|    reward             | -8.046196 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 223       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 11       |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 2414     |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -689     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 782      |\n",
      "|    reward             | 0.811196 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 2.02     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 11          |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 2458        |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -690        |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | 408         |\n",
      "|    reward             | -0.56251156 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 1.49        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 2501      |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -691      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | 104       |\n",
      "|    reward             | 13.685724 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 53.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 2545       |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -691       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -6.61e+03  |\n",
      "|    reward             | -1.3215352 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 218        |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 11            |\n",
      "|    iterations         | 6100          |\n",
      "|    time_elapsed       | 2589          |\n",
      "|    total_timesteps    | 30500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -691          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6099          |\n",
      "|    policy_loss        | 1.1e+03       |\n",
      "|    reward             | -0.0083035715 |\n",
      "|    std                | 1.05          |\n",
      "|    value_loss         | 4.33          |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 11       |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 2633     |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -692     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 3.4e+03  |\n",
      "|    reward             | 4.586284 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 29.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 2677      |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -692      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | -470      |\n",
      "|    reward             | 3.4452255 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.91      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 2721       |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -692       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | 4.58e+03   |\n",
      "|    reward             | -18.613441 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 311        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 2766       |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -693       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 1.07e+03   |\n",
      "|    reward             | -2.5887787 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 3.9        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 2811       |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -693       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | 346        |\n",
      "|    reward             | -1.0747253 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 2857       |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -693       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | 1.03e+03   |\n",
      "|    reward             | -0.6922803 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.41       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 2901      |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -694      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -2.21e+03 |\n",
      "|    reward             | -6.163337 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 19        |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 11           |\n",
      "|    iterations         | 6900         |\n",
      "|    time_elapsed       | 2945         |\n",
      "|    total_timesteps    | 34500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -694         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6899         |\n",
      "|    policy_loss        | -2.33e+03    |\n",
      "|    reward             | -0.031756524 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 17.3         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 2988      |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -695      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 2.7e+03   |\n",
      "|    reward             | 2.4326072 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 16.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 3030      |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -695      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -308      |\n",
      "|    reward             | 1.014419  |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.871     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 3075      |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -695      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | 6.41e+03  |\n",
      "|    reward             | -2.388616 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 113       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 11          |\n",
      "|    iterations         | 7300        |\n",
      "|    time_elapsed       | 3119        |\n",
      "|    total_timesteps    | 36500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -695        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7299        |\n",
      "|    policy_loss        | -1.78e+03   |\n",
      "|    reward             | -0.28157407 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 7.24        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 3165       |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -695       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | 996        |\n",
      "|    reward             | -0.5662266 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.52       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 3206       |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -695       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -242       |\n",
      "|    reward             | -0.9704625 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.74       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 11        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 3247      |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -695      |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -2.66e+04 |\n",
      "|    reward             | 16.24508  |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.61e+03  |\n",
      "-------------------------------------\n",
      "day: 2013, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3409098.48\n",
      "total_reward: 2409098.48\n",
      "total_cost: 10863.92\n",
      "total_trades: 472201\n",
      "Sharpe: 0.694\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 11         |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 3289       |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -696       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | -47.6      |\n",
      "|    reward             | -1.7004766 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.16       |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 训练 A2C 模型\n",
    "if if_using_a2c:\n",
    "    print(\"\\n======== 开始训练 A2C 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # A2C特定参数 - GPU优化\n",
    "    A2C_PARAMS = {\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"learning_rate\": 0.0007,\n",
    "        \"device\": gpu_device,\n",
    "    }\n",
    "\n",
    "    model_a2c = agent.get_model(\"a2c\", model_kwargs=A2C_PARAMS)\n",
    "\n",
    "    # 设置日志记录\n",
    "    tmp_path = RESULTS_DIR + \"/a2c\"\n",
    "    new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_a2c.set_logger(new_logger_a2c)\n",
    "\n",
    "    # 添加检查点回调\n",
    "    from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "    # 每10000步保存一次检查点\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=10000,\n",
    "        save_path=TRAINED_MODEL_DIR + \"/checkpoints/a2c/\",\n",
    "        name_prefix=\"a2c_model\",\n",
    "        save_replay_buffer=True,\n",
    "        save_vecnormalize=True,\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    train_start_time = time.time()\n",
    "    print(f\"开始训练，总步数: {a2c_timesteps}\")\n",
    "    trained_a2c = agent.train_model(\n",
    "        model=model_a2c,\n",
    "        tb_log_name=\"a2c\",\n",
    "        total_timesteps=a2c_timesteps,\n",
    "        callback=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"A2C 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存最终模型\n",
    "    trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c_20150101~20250101\")\n",
    "    print(f\"模型已保存至 {TRAINED_MODEL_DIR}/agent_a2c_20150101~20250101\")\n",
    "    print(f\"检查点保存在 {TRAINED_MODEL_DIR}/checkpoints/a2c/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 DDPG 模型\n",
    "if if_using_ddpg:\n",
    "    print(\"\\n======== 开始训练 DDPG 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # DDPG特定参数 - 内存优化\n",
    "    DDPG_PARAMS = {\n",
    "        \"buffer_size\": 10000,  # 减小缓冲区大小（原50000）\n",
    "        \"learning_rate\": 0.0005,\n",
    "        \"batch_size\": 32,  # 减小批量大小（原128/64）\n",
    "        \"device\": gpu_device,\n",
    "    }\n",
    "\n",
    "    model_ddpg = agent.get_model(\"ddpg\", model_kwargs=DDPG_PARAMS)\n",
    "\n",
    "    # 设置日志记录 - 减少日志频率\n",
    "    tmp_path = RESULTS_DIR + \"/ddpg\"\n",
    "    new_logger_ddpg = configure(\n",
    "        tmp_path, [\"stdout\", \"csv\"]\n",
    "    )  # 移除tensorboard减轻内存负担\n",
    "    model_ddpg.set_logger(new_logger_ddpg)\n",
    "\n",
    "    # 训练模型 - 分阶段训练以减轻内存压力\n",
    "    train_start_time = time.time()\n",
    "    total_steps = ddpg_timesteps\n",
    "    steps_per_stage = 10000  # 每阶段训练步数\n",
    "    stages = total_steps // steps_per_stage\n",
    "\n",
    "    print(f\"开始分阶段训练DDPG，总步数: {total_steps}，分为{stages}个阶段\")\n",
    "\n",
    "    # 手动垃圾回收\n",
    "    import gc\n",
    "\n",
    "    for stage in range(stages):\n",
    "        print(f\"阶段 {stage+1}/{stages}，训练步数: {steps_per_stage}\")\n",
    "        model_ddpg = agent.train_model(\n",
    "            model=model_ddpg,\n",
    "            tb_log_name=f\"ddpg_stage_{stage}\",\n",
    "            total_timesteps=steps_per_stage,\n",
    "        )\n",
    "\n",
    "        # 强制垃圾回收\n",
    "        gc.collect()\n",
    "\n",
    "        # 每阶段保存一次模型，避免全部失败\n",
    "        if (stage + 1) % 2 == 0:\n",
    "            checkpoint_path = f\"{TRAINED_MODEL_DIR}/agent_ddpg_checkpoint_{stage+1}\"\n",
    "            model_ddpg.save(checkpoint_path)\n",
    "            print(f\"保存阶段性检查点: {checkpoint_path}\")\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"DDPG 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存最终模型\n",
    "    trained_ddpg = model_ddpg  # 使用最终训练好的模型\n",
    "    trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg_20150101~20250101\")\n",
    "    print(f\"模型已保存至 {TRAINED_MODEL_DIR}/agent_ddpg_20150101~20250101\")\n",
    "\n",
    "    # 清理内存\n",
    "    gc.collect()\n",
    "    print(\"内存已清理\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 PPO 模型 - 分阶段版本\n",
    "if if_using_ppo:\n",
    "    print(\"\\n======== 开始训练 PPO 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # PPO 特定参数 - GPU优化\n",
    "    PPO_PARAMS = {\n",
    "        \"n_steps\": 2048,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"learning_rate\": 0.00025,\n",
    "        \"batch_size\": 256 if use_cuda else 128,  # GPU上使用更大batch_size\n",
    "        \"device\": cpu_device,  # 使用GPU\n",
    "    }\n",
    "    model_ppo = agent.get_model(\"ppo\", model_kwargs=PPO_PARAMS)\n",
    "\n",
    "    # 设置日志记录\n",
    "    tmp_path = RESULTS_DIR + \"/ppo\"\n",
    "    new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\"])  # 移除tensorboard减轻负担\n",
    "    model_ppo.set_logger(new_logger_ppo)\n",
    "\n",
    "    # 分阶段训练设置\n",
    "    import gc\n",
    "\n",
    "    train_start_time = time.time()\n",
    "    total_steps = ppo_timesteps\n",
    "    steps_per_stage = 20000  # 每阶段训练步数\n",
    "    stages = total_steps // steps_per_stage\n",
    "\n",
    "    print(f\"开始分阶段训练PPO，总步数: {total_steps}，分为{stages}个阶段\")\n",
    "\n",
    "    try:\n",
    "        for stage in range(stages):\n",
    "            print(f\"阶段 {stage+1}/{stages}，训练步数: {steps_per_stage}\")\n",
    "            model_ppo = agent.train_model(\n",
    "                model=model_ppo,\n",
    "                tb_log_name=f\"ppo_stage_{stage}\",\n",
    "                total_timesteps=steps_per_stage,\n",
    "            )\n",
    "\n",
    "            # 强制垃圾回收\n",
    "            gc.collect()\n",
    "\n",
    "            # 每阶段保存一次模型\n",
    "            checkpoint_path = f\"{TRAINED_MODEL_DIR}/agent_ppo_checkpoint_{stage+1}\"\n",
    "            model_ppo.save(checkpoint_path)\n",
    "            print(f\"保存阶段性检查点: {checkpoint_path}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n训练被用户中断，保存当前模型...\")\n",
    "        interrupt_path = f\"{TRAINED_MODEL_DIR}/agent_ppo_interrupted\"\n",
    "        model_ppo.save(interrupt_path)\n",
    "        print(f\"中断模型已保存至: {interrupt_path}\")\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"PPO 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存最终模型\n",
    "    trained_ppo = model_ppo\n",
    "    trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo_20150101~20250101\")\n",
    "    print(f\"最终模型已保存至 {TRAINED_MODEL_DIR}/agent_ppo_20150101~20250101\")\n",
    "\n",
    "    # 清理内存\n",
    "    gc.collect()\n",
    "    print(\"内存已清理\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD3 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 TD3 模型\n",
    "if if_using_td3:\n",
    "    print(\"\\n======== 开始训练 TD3 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # TD3 特定参数 - GPU优化\n",
    "    TD3_PARAMS = {\n",
    "        \"batch_size\": 256 if use_cuda else 100,\n",
    "        \"buffer_size\": 1000000,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"device\": gpu_device,  # 使用GPU\n",
    "    }\n",
    "    model_td3 = agent.get_model(\"td3\", model_kwargs=TD3_PARAMS)\n",
    "\n",
    "    # 设置日志记录\n",
    "    tmp_path = RESULTS_DIR + \"/td3\"\n",
    "    new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_td3.set_logger(new_logger_td3)\n",
    "\n",
    "    # 训练模型\n",
    "    train_start_time = time.time()\n",
    "    print(f\"开始训练，总步数: {td3_timesteps}\")\n",
    "    trained_td3 = agent.train_model(\n",
    "        model=model_td3, tb_log_name=\"td3\", total_timesteps=td3_timesteps\n",
    "    )\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"TD3 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存模型\n",
    "    trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3_20150101~20250101\")\n",
    "    print(f\"模型已保存至 {TRAINED_MODEL_DIR}/agent_td3_20150101~20250101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 SAC 模型\n",
    "if if_using_sac:\n",
    "    print(\"\\n======== 开始训练 SAC 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # SAC 特定参数 - GPU优化\n",
    "    SAC_PARAMS = {\n",
    "        \"batch_size\": 256 if use_cuda else 128,\n",
    "        \"buffer_size\": 300000,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"learning_starts\": 100,\n",
    "        \"ent_coef\": \"auto_0.1\",\n",
    "        \"device\": gpu_device,  # 使用GPU\n",
    "    }\n",
    "    model_sac = agent.get_model(\"sac\", model_kwargs=SAC_PARAMS)\n",
    "\n",
    "    # 设置日志记录\n",
    "    tmp_path = RESULTS_DIR + \"/sac\"\n",
    "    new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_sac.set_logger(new_logger_sac)\n",
    "\n",
    "    # 训练模型\n",
    "    train_start_time = time.time()\n",
    "    print(f\"开始训练，总步数: {sac_timesteps}\")\n",
    "    trained_sac = agent.train_model(\n",
    "        model=model_sac, tb_log_name=\"sac\", total_timesteps=sac_timesteps\n",
    "    )\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"SAC 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存模型\n",
    "    trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac_20150101~20250101\")\n",
    "    print(f\"模型已保存至 {TRAINED_MODEL_DIR}/agent_sac_20150101~20250101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练过程中的模型性能\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def visualize_training_results(model_name):\n",
    "    \"\"\"可视化模型训练过程中的奖励和损失\"\"\"\n",
    "    csv_path = os.path.join(RESULTS_DIR, model_name, \"*.monitor.csv\")\n",
    "    csv_files = glob.glob(csv_path)\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"未找到 {model_name} 的训练记录文件\")\n",
    "        return\n",
    "\n",
    "    # 读取训练记录\n",
    "    data = pd.read_csv(csv_files[0], skiprows=1)\n",
    "\n",
    "    # 绘制奖励变化趋势\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data[\"r\"], label=\"奖励\", alpha=0.3)\n",
    "    plt.plot(data[\"r\"].rolling(window=100).mean(), label=\"奖励均值(窗口=100)\")\n",
    "    plt.title(f\"{model_name} 模型训练奖励\")\n",
    "    plt.xlabel(\"训练步数\")\n",
    "    plt.ylabel(\"奖励\")\n",
    "    plt.legend()\n",
    "    # 确保results目录存在\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    plt.savefig(f\"results/{model_name}_reward.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 可视化训练结果\n",
    "trained_models = []\n",
    "if if_using_a2c:\n",
    "    visualize_training_results(\"a2c\")\n",
    "    trained_models.append(\"A2C\")\n",
    "if if_using_ddpg:\n",
    "    visualize_training_results(\"ddpg\")\n",
    "    trained_models.append(\"DDPG\")\n",
    "if if_using_ppo:\n",
    "    visualize_training_results(\"ppo\")\n",
    "    trained_models.append(\"PPO\")\n",
    "if if_using_td3:\n",
    "    visualize_training_results(\"td3\")\n",
    "    trained_models.append(\"TD3\")\n",
    "if if_using_sac:\n",
    "    visualize_training_results(\"sac\")\n",
    "    trained_models.append(\"SAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结训练结果\n",
    "print(\"\\n======== 训练完成 ========\")\n",
    "print(f\"使用设备: {'GPU (CUDA)' if use_cuda else 'CPU'}\")\n",
    "print(f\"训练的模型: {', '.join(trained_models)}\")\n",
    "print(f\"所有模型已保存至 {TRAINED_MODEL_DIR} 目录\")\n",
    "print(\"\\n下一步: 运行 back_test.ipynb 来评估训练好的模型表现\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
