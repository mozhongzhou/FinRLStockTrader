{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号\n",
    "import torch\n",
    "import time\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "print(\"检查GPU可用性...\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    cuda_device_count = torch.cuda.device_count()\n",
    "    cuda_device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"✓ 发现 {cuda_device_count} 个可用的GPU设备\")\n",
    "    print(f\"✓ 当前使用: {cuda_device_name}\")\n",
    "else:\n",
    "    print(\"✗ 未发现可用的GPU，将使用CPU进行训练\")\n",
    "\n",
    "# 确保模型保存目录存在\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])\n",
    "# 设置随机种子以确保结果可复现\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预处理后的训练数据\n",
    "processed_data_file = \"data/processed_data/train_data_small.csv\"\n",
    "\n",
    "# 检查文件是否存在\n",
    "if not os.path.exists(processed_data_file):\n",
    "    raise FileNotFoundError(\n",
    "        f\"找不到处理后的数据文件: {processed_data_file}，请先运行 process_data.ipynb\"\n",
    "    )\n",
    "\n",
    "# 加载训练数据\n",
    "train = pd.read_csv(processed_data_file)\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = [\"\"]\n",
    "\n",
    "print(f\"加载训练数据: {len(train)} 条记录\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建交易环节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建交易环境的参数\n",
    "# 计算环境参数\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
    "print(f\"股票数量: {stock_dimension}, 状态空间维度: {state_space}\")\n",
    "\n",
    "# 设置环境参数\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension  # 交易成本 0.1%\n",
    "num_stock_shares = [0] * stock_dimension  # 初始持有股票数量\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 503,  # 最大持仓数量\n",
    "    \"initial_amount\": 1000000,  # 初始资金\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "}\n",
    "\n",
    "# 构建交易环境\n",
    "e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(f\"环境类型: {type(env_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算法选择与GPU/CPU设置\n",
    "# 设置为 True 选择使用相应算法\n",
    "if_using_a2c = False\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True\n",
    "\n",
    "# GPU相关设置\n",
    "if use_cuda:\n",
    "    # 根据算法特性分配设备\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    gpu_device = torch.device(\"cuda\")\n",
    "\n",
    "    print(f\"CPU设备: {cpu_device}\")\n",
    "    print(f\"GPU设备: {gpu_device}\")\n",
    "else:\n",
    "    # 如果没有GPU，所有模型都使用CPU\n",
    "    cpu_device = gpu_device = torch.device(\"cpu\")\n",
    "    print(\"未检测到GPU，所有模型将使用CPU\")\n",
    "\n",
    "# 因为有GPU加速，可以适当增加训练步数提高性能\n",
    "a2c_timesteps = 50000  # 即使有GPU也用较少步数，因为在CPU上运行\n",
    "ddpg_timesteps = 100000 if use_cuda else 50000\n",
    "ppo_timesteps = 100000 if use_cuda else 50000\n",
    "td3_timesteps = 80000 if use_cuda else 30000\n",
    "sac_timesteps = 80000 if use_cuda else 30000\n",
    "\n",
    "print(\"选中的算法及其训练设备:\")\n",
    "print(f\"A2C: {'✓' if if_using_a2c else '✗'} (设备: CPU)\")\n",
    "print(f\"DDPG: {'✓' if if_using_ddpg else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")\n",
    "print(f\"PPO: {'✓' if if_using_ppo else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")\n",
    "print(f\"TD3: {'✓' if if_using_td3 else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")\n",
    "print(f\"SAC: {'✓' if if_using_sac else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 A2C 模型\n",
    "if if_using_a2c:\n",
    "    print(\"\\n======== 开始训练 A2C 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # A2C特定参数 - GPU优化\n",
    "    A2C_PARAMS = {\n",
    "        \"n_steps\": 5,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"learning_rate\": 0.0007,\n",
    "        \"device\": cpu_device,  # 使用GPU\n",
    "    }\n",
    "\n",
    "    model_a2c = agent.get_model(\"a2c\", model_kwargs=A2C_PARAMS)\n",
    "\n",
    "    # 设置日志记录\n",
    "    tmp_path = RESULTS_DIR + \"/a2c\"\n",
    "    new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_a2c.set_logger(new_logger_a2c)\n",
    "\n",
    "    # 训练模型\n",
    "    train_start_time = time.time()\n",
    "    print(f\"开始训练，总步数: {a2c_timesteps}\")\n",
    "    trained_a2c = agent.train_model(\n",
    "        model=model_a2c, tb_log_name=\"a2c\", total_timesteps=a2c_timesteps\n",
    "    )\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"A2C 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存模型\n",
    "    trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\")\n",
    "    print(f\"模型已保存至 {TRAINED_MODEL_DIR}/agent_a2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 DDPG 模型\n",
    "if if_using_ddpg:\n",
    "    print(\"\\n======== 开始训练 DDPG 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # DDPG特定参数 - 内存优化\n",
    "    DDPG_PARAMS = {\n",
    "        \"buffer_size\": 10000,  # 减小缓冲区大小（原50000）\n",
    "        \"learning_rate\": 0.0005,\n",
    "        \"batch_size\": 32,  # 减小批量大小（原128/64）\n",
    "        \"device\": gpu_device,\n",
    "    }\n",
    "\n",
    "    model_ddpg = agent.get_model(\"ddpg\", model_kwargs=DDPG_PARAMS)\n",
    "\n",
    "    # 设置日志记录 - 减少日志频率\n",
    "    tmp_path = RESULTS_DIR + \"/ddpg\"\n",
    "    new_logger_ddpg = configure(\n",
    "        tmp_path, [\"stdout\", \"csv\"]\n",
    "    )  # 移除tensorboard减轻内存负担\n",
    "    model_ddpg.set_logger(new_logger_ddpg)\n",
    "\n",
    "    # 训练模型 - 分阶段训练以减轻内存压力\n",
    "    train_start_time = time.time()\n",
    "    total_steps = ddpg_timesteps\n",
    "    steps_per_stage = 10000  # 每阶段训练步数\n",
    "    stages = total_steps // steps_per_stage\n",
    "\n",
    "    print(f\"开始分阶段训练DDPG，总步数: {total_steps}，分为{stages}个阶段\")\n",
    "\n",
    "    # 手动垃圾回收\n",
    "    import gc\n",
    "\n",
    "    for stage in range(stages):\n",
    "        print(f\"阶段 {stage+1}/{stages}，训练步数: {steps_per_stage}\")\n",
    "        model_ddpg = agent.train_model(\n",
    "            model=model_ddpg,\n",
    "            tb_log_name=f\"ddpg_stage_{stage}\",\n",
    "            total_timesteps=steps_per_stage,\n",
    "        )\n",
    "\n",
    "        # 强制垃圾回收\n",
    "        gc.collect()\n",
    "\n",
    "        # 每阶段保存一次模型，避免全部失败\n",
    "        if (stage + 1) % 2 == 0:\n",
    "            checkpoint_path = f\"{TRAINED_MODEL_DIR}/agent_ddpg_checkpoint_{stage+1}\"\n",
    "            model_ddpg.save(checkpoint_path)\n",
    "            print(f\"保存阶段性检查点: {checkpoint_path}\")\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"DDPG 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存最终模型\n",
    "    trained_ddpg = model_ddpg  # 使用最终训练好的模型\n",
    "    trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\")\n",
    "    print(f\"模型已保存至 {TRAINED_MODEL_DIR}/agent_ddpg\")\n",
    "\n",
    "    # 清理内存\n",
    "    gc.collect()\n",
    "    print(\"内存已清理\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 PPO 模型 - 分阶段版本\n",
    "if if_using_ppo:\n",
    "    print(\"\\n======== 开始训练 PPO 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # PPO 特定参数 - GPU优化\n",
    "    PPO_PARAMS = {\n",
    "        \"n_steps\": 2048,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"learning_rate\": 0.00025,\n",
    "        \"batch_size\": 256 if use_cuda else 128,  # GPU上使用更大batch_size\n",
    "        \"device\": gpu_device,  # 使用GPU\n",
    "    }\n",
    "    model_ppo = agent.get_model(\"ppo\", model_kwargs=PPO_PARAMS)\n",
    "\n",
    "    # 设置日志记录\n",
    "    tmp_path = RESULTS_DIR + \"/ppo\"\n",
    "    new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\"])  # 移除tensorboard减轻负担\n",
    "    model_ppo.set_logger(new_logger_ppo)\n",
    "\n",
    "    # 分阶段训练设置\n",
    "    import gc\n",
    "\n",
    "    train_start_time = time.time()\n",
    "    total_steps = ppo_timesteps\n",
    "    steps_per_stage = 20000  # 每阶段训练步数\n",
    "    stages = total_steps // steps_per_stage\n",
    "\n",
    "    print(f\"开始分阶段训练PPO，总步数: {total_steps}，分为{stages}个阶段\")\n",
    "\n",
    "    try:\n",
    "        for stage in range(stages):\n",
    "            print(f\"阶段 {stage+1}/{stages}，训练步数: {steps_per_stage}\")\n",
    "            model_ppo = agent.train_model(\n",
    "                model=model_ppo,\n",
    "                tb_log_name=f\"ppo_stage_{stage}\",\n",
    "                total_timesteps=steps_per_stage,\n",
    "            )\n",
    "\n",
    "            # 强制垃圾回收\n",
    "            gc.collect()\n",
    "\n",
    "            # 每阶段保存一次模型\n",
    "            checkpoint_path = f\"{TRAINED_MODEL_DIR}/agent_ppo_checkpoint_{stage+1}\"\n",
    "            model_ppo.save(checkpoint_path)\n",
    "            print(f\"保存阶段性检查点: {checkpoint_path}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n训练被用户中断，保存当前模型...\")\n",
    "        interrupt_path = f\"{TRAINED_MODEL_DIR}/agent_ppo_interrupted\"\n",
    "        model_ppo.save(interrupt_path)\n",
    "        print(f\"中断模型已保存至: {interrupt_path}\")\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"PPO 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存最终模型\n",
    "    trained_ppo = model_ppo\n",
    "    trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\")\n",
    "    print(f\"最终模型已保存至 {TRAINED_MODEL_DIR}/agent_ppo\")\n",
    "\n",
    "    # 清理内存\n",
    "    gc.collect()\n",
    "    print(\"内存已清理\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD3 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 TD3 模型\n",
    "if if_using_td3:\n",
    "    print(\"\\n======== 开始训练 TD3 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # TD3 特定参数 - GPU优化\n",
    "    TD3_PARAMS = {\n",
    "        \"batch_size\": 256 if use_cuda else 100,\n",
    "        \"buffer_size\": 1000000,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"device\": gpu_device,  # 使用GPU\n",
    "    }\n",
    "    model_td3 = agent.get_model(\"td3\", model_kwargs=TD3_PARAMS)\n",
    "\n",
    "    # 设置日志记录\n",
    "    tmp_path = RESULTS_DIR + \"/td3\"\n",
    "    new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_td3.set_logger(new_logger_td3)\n",
    "\n",
    "    # 训练模型\n",
    "    train_start_time = time.time()\n",
    "    print(f\"开始训练，总步数: {td3_timesteps}\")\n",
    "    trained_td3 = agent.train_model(\n",
    "        model=model_td3, tb_log_name=\"td3\", total_timesteps=td3_timesteps\n",
    "    )\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"TD3 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存模型\n",
    "    trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\")\n",
    "    print(f\"模型已保存至 {TRAINED_MODEL_DIR}/agent_td3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 SAC 模型\n",
    "if if_using_sac:\n",
    "    print(\"\\n======== 开始训练 SAC 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "\n",
    "    # SAC 特定参数 - GPU优化\n",
    "    SAC_PARAMS = {\n",
    "        \"batch_size\": 256 if use_cuda else 128,\n",
    "        \"buffer_size\": 300000,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"learning_starts\": 100,\n",
    "        \"ent_coef\": \"auto_0.1\",\n",
    "        \"device\": gpu_device,  # 使用GPU\n",
    "    }\n",
    "    model_sac = agent.get_model(\"sac\", model_kwargs=SAC_PARAMS)\n",
    "\n",
    "    # 设置日志记录\n",
    "    tmp_path = RESULTS_DIR + \"/sac\"\n",
    "    new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_sac.set_logger(new_logger_sac)\n",
    "\n",
    "    # 训练模型\n",
    "    train_start_time = time.time()\n",
    "    print(f\"开始训练，总步数: {sac_timesteps}\")\n",
    "    trained_sac = agent.train_model(\n",
    "        model=model_sac, tb_log_name=\"sac\", total_timesteps=sac_timesteps\n",
    "    )\n",
    "\n",
    "    # 计算训练时间\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"SAC 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "\n",
    "    # 保存模型\n",
    "    trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\")\n",
    "    print(f\"模型已保存至 {TRAINED_MODEL_DIR}/agent_sac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练过程中的模型性能\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def visualize_training_results(model_name):\n",
    "    \"\"\"可视化模型训练过程中的奖励和损失\"\"\"\n",
    "    csv_path = os.path.join(RESULTS_DIR, model_name, \"*.monitor.csv\")\n",
    "    csv_files = glob.glob(csv_path)\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"未找到 {model_name} 的训练记录文件\")\n",
    "        return\n",
    "\n",
    "    # 读取训练记录\n",
    "    data = pd.read_csv(csv_files[0], skiprows=1)\n",
    "\n",
    "    # 绘制奖励变化趋势\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data[\"r\"], label=\"奖励\", alpha=0.3)\n",
    "    plt.plot(data[\"r\"].rolling(window=100).mean(), label=\"奖励均值(窗口=100)\")\n",
    "    plt.title(f\"{model_name} 模型训练奖励\")\n",
    "    plt.xlabel(\"训练步数\")\n",
    "    plt.ylabel(\"奖励\")\n",
    "    plt.legend()\n",
    "    # 确保results目录存在\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    plt.savefig(f\"results/{model_name}_reward.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 可视化训练结果\n",
    "trained_models = []\n",
    "if if_using_a2c:\n",
    "    visualize_training_results(\"a2c\")\n",
    "    trained_models.append(\"A2C\")\n",
    "if if_using_ddpg:\n",
    "    visualize_training_results(\"ddpg\")\n",
    "    trained_models.append(\"DDPG\")\n",
    "if if_using_ppo:\n",
    "    visualize_training_results(\"ppo\")\n",
    "    trained_models.append(\"PPO\")\n",
    "if if_using_td3:\n",
    "    visualize_training_results(\"td3\")\n",
    "    trained_models.append(\"TD3\")\n",
    "if if_using_sac:\n",
    "    visualize_training_results(\"sac\")\n",
    "    trained_models.append(\"SAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结训练结果\n",
    "print(\"\\n======== 训练完成 ========\")\n",
    "print(f\"使用设备: {'GPU (CUDA)' if use_cuda else 'CPU'}\")\n",
    "print(f\"训练的模型: {', '.join(trained_models)}\")\n",
    "print(f\"所有模型已保存至 {TRAINED_MODEL_DIR} 目录\")\n",
    "print(\"\\n下一步: 运行 back_test.ipynb 来评估训练好的模型表现\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
