{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install finrl library\n",
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查GPU可用性...\n",
      "✓ 发现 1 个可用的GPU设备\n",
      "✓ 当前使用: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    BaseCallback,\n",
    "    CheckpointCallback,\n",
    "    EvalCallback,\n",
    "    CallbackList,\n",
    ")\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]  # 用来正常显示中文标签\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # 用来正常显示负号\n",
    "\n",
    "print(\"检查GPU可用性...\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    cuda_device_count = torch.cuda.device_count()\n",
    "    cuda_device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"✓ 发现 {cuda_device_count} 个可用的GPU设备\")\n",
    "    print(f\"✓ 当前使用: {cuda_device_name}\")\n",
    "else:\n",
    "    print(\"✗ 未发现可用的GPU，将使用CPU进行训练\")\n",
    "\n",
    "# 确保模型保存目录存在\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])\n",
    "# 设置随机种子以确保结果可复现\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m      7\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m找不到处理后的数据文件: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_data_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m，请先运行 process_data.ipynb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 加载训练数据\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m train = train.set_index(train.columns[\u001b[32m0\u001b[39m])\n\u001b[32m     13\u001b[39m train.index.names = [\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\DevelopentTools\\Anaconda3\\envs\\FinRL\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\DevelopentTools\\Anaconda3\\envs\\FinRL\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\DevelopentTools\\Anaconda3\\envs\\FinRL\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\DevelopentTools\\Anaconda3\\envs\\FinRL\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# 加载预处理后的训练数据\n",
    "processed_data_file = \"data/processed_data/train_data_20150101~20250101.csv\"\n",
    "\n",
    "# 检查文件是否存在\n",
    "if not os.path.exists(processed_data_file):\n",
    "    raise FileNotFoundError(\n",
    "        f\"找不到处理后的数据文件: {processed_data_file}，请先运行 process_data.ipynb\"\n",
    "    )\n",
    "\n",
    "# 加载训练数据\n",
    "train = pd.read_csv(processed_data_file)\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = [\"\"]\n",
    "\n",
    "\n",
    "# 在加载数据后添加以下代码来减少股票数量\n",
    "def reduce_stock_dataset(df, top_n_stocks=15):\n",
    "    \"\"\"仅保留市值最大的top_n_stocks支股票\"\"\"\n",
    "    print(f\"原始股票数量: {len(df.tic.unique())}\")\n",
    "\n",
    "    # 计算每支股票的平均成交量，作为选择大市值股票的简单依据\n",
    "    stock_volume = df.groupby(\"tic\")[\"volume\"].mean().sort_values(ascending=False)\n",
    "    selected_stocks = stock_volume.head(top_n_stocks).index.tolist()\n",
    "\n",
    "    # 过滤数据\n",
    "    reduced_df = df[df.tic.isin(selected_stocks)].copy()\n",
    "    print(f\"缩减后股票数量: {len(reduced_df.tic.unique())}\")\n",
    "    return reduced_df\n",
    "\n",
    "\n",
    "# 应用缩减\n",
    "train = reduce_stock_dataset(train, top_n_stocks=15)  # 减少到15只\n",
    "\n",
    "print(f\"加载训练数据: {len(train)} 条记录\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建交易环节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建交易环境的参数\n",
    "# 计算环境参数\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
    "print(f\"股票数量: {stock_dimension}, 状态空间维度: {state_space}\")\n",
    "\n",
    "# 设置环境参数\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension  # 交易成本\n",
    "num_stock_shares = [0] * stock_dimension  # 初始持有股票数量\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,  # 每个交易周期的最大交易次数\n",
    "    \"initial_amount\": 1000000,  # 初始资金\n",
    "    \"num_stock_shares\": num_stock_shares,  # 初始持有股票数量\n",
    "    \"buy_cost_pct\": buy_cost_list,  # 买入成本\n",
    "    \"sell_cost_pct\": sell_cost_list,  # 卖出成本\n",
    "    \"state_space\": state_space,  # 状态空间\n",
    "    \"stock_dim\": stock_dimension,  # 股票数量\n",
    "    \"tech_indicator_list\": INDICATORS,  # 技术指标列表\n",
    "    \"action_space\": stock_dimension,  # 动作空间\n",
    "    \"reward_scaling\": 1e-3,  # 奖励缩放\n",
    "}\n",
    "\n",
    "# 构建交易环境\n",
    "e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(f\"环境类型: {type(env_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 早停机制 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 早停机制\n",
    "class EarlyStoppingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=1000, patience=5, min_delta=0.01, verbose=1):\n",
    "        super(EarlyStoppingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.no_improvement_count = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            if len(self.model.ep_info_buffer) > 0:\n",
    "                mean_reward = np.mean(\n",
    "                    [ep_info[\"r\"] for ep_info in self.model.ep_info_buffer]\n",
    "                )\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"步数: {self.num_timesteps}, 平均奖励: {mean_reward:.2f}\")\n",
    "                if mean_reward - self.best_mean_reward < self.min_delta:\n",
    "                    self.no_improvement_count += 1\n",
    "                    if self.verbose > 0:\n",
    "                        print(\n",
    "                            f\"无显著改进: {self.no_improvement_count}/{self.patience}\"\n",
    "                        )\n",
    "                    if self.no_improvement_count >= self.patience:\n",
    "                        if self.verbose > 0:\n",
    "                            print(\"早停: 已连续多次无显著改进\")\n",
    "                        return False\n",
    "                else:\n",
    "                    self.no_improvement_count = 0\n",
    "                    self.best_mean_reward = mean_reward\n",
    "        return True\n",
    "\n",
    "\n",
    "def setup_callbacks(model_name, env_train, eval_freq=5000, check_freq=1000):\n",
    "    os.makedirs(f\"{RESULTS_DIR}/eval/{model_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"{TRAINED_MODEL_DIR}/best/{model_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"{TRAINED_MODEL_DIR}/checkpoints/{model_name}\", exist_ok=True)\n",
    "\n",
    "    eval_env = env_train\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=f\"{TRAINED_MODEL_DIR}/best/{model_name}/\",\n",
    "        log_path=f\"{RESULTS_DIR}/eval/{model_name}/\",\n",
    "        eval_freq=eval_freq,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose=1,\n",
    "    )\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=eval_freq,\n",
    "        save_path=f\"{TRAINED_MODEL_DIR}/checkpoints/{model_name}/\",\n",
    "        name_prefix=f\"{model_name}_model\",\n",
    "        save_replay_buffer=True,\n",
    "        save_vecnormalize=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        check_freq=check_freq, patience=5, min_delta=0.01, verbose=1\n",
    "    )\n",
    "    return CallbackList([checkpoint_callback, eval_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算法选择与GPU/CPU设置\n",
    "# 设置为 True 选择使用相应算法\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True\n",
    "\n",
    "# GPU相关设置\n",
    "if use_cuda:\n",
    "    # 根据算法特性分配设备\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    gpu_device = torch.device(\"cuda\")\n",
    "\n",
    "    print(f\"CPU设备: {cpu_device}\")\n",
    "    print(f\"GPU设备: {gpu_device}\")\n",
    "else:\n",
    "    # 如果没有GPU，所有模型都使用CPU\n",
    "    cpu_device = gpu_device = torch.device(\"cpu\")\n",
    "    print(\"未检测到GPU，所有模型将使用CPU\")\n",
    "\n",
    "\n",
    "a2c_timesteps = 20000\n",
    "ddpg_timesteps = 20000\n",
    "ppo_timesteps = 20000\n",
    "td3_timesteps = 20000\n",
    "sac_timesteps = 20000\n",
    "\n",
    "print(\"选中的算法及其训练设备:\")\n",
    "print(f\"A2C: {'✓' if if_using_a2c else '✗'} (设备: CPU)\")\n",
    "print(f\"DDPG: {'✓' if if_using_ddpg else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")\n",
    "print(f\"PPO: {'✓' if if_using_ppo else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")\n",
    "print(f\"TD3: {'✓' if if_using_td3 else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")\n",
    "print(f\"SAC: {'✓' if if_using_sac else '✗'} (设备: {'GPU' if use_cuda else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C 模型\n",
    "if if_using_a2c:\n",
    "    print(\"\\n======== 开始训练 A2C 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    A2C_PARAMS = {\n",
    "        \"n_steps\": 16,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"device\": cpu_device,\n",
    "    }\n",
    "    model_a2c = agent.get_model(\"a2c\", model_kwargs=A2C_PARAMS)\n",
    "    tmp_path = RESULTS_DIR + \"/a2c\"\n",
    "    new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_a2c.set_logger(new_logger_a2c)\n",
    "    callbacks = setup_callbacks(\"a2c\", env_train, eval_freq=5000, check_freq=1000)\n",
    "\n",
    "    train_start_time = time.time()\n",
    "    print(f\"开始训练，总步数: {a2c_timesteps}\")\n",
    "    trained_a2c = model_a2c.learn(\n",
    "        total_timesteps=a2c_timesteps, callback=callbacks, tb_log_name=\"a2c\"\n",
    "    )\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"A2C 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "    trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c_20150101~20250101\")\n",
    "    print(f\"最终模型已保存至 {TRAINED_MODEL_DIR}/agent_a2c_20150101~20250101\")\n",
    "    print(f\"检查点保存在 {TRAINED_MODEL_DIR}/checkpoints/a2c/\")\n",
    "    print(f\"最佳模型已保存至 {TRAINED_MODEL_DIR}/best/a2c/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG 模型\n",
    "if if_using_ddpg:\n",
    "    print(\"\\n======== 开始训练 DDPG 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    DDPG_PARAMS = {\n",
    "        \"buffer_size\": 10000,\n",
    "        \"learning_rate\": 0.0005,\n",
    "        \"batch_size\": 128,\n",
    "        \"device\": gpu_device,\n",
    "    }\n",
    "    model_ddpg = agent.get_model(\"ddpg\", model_kwargs=DDPG_PARAMS)\n",
    "    tmp_path = RESULTS_DIR + \"/ddpg\"\n",
    "    new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_ddpg.set_logger(new_logger_ddpg)\n",
    "    callbacks = setup_callbacks(\"ddpg\", env_train, eval_freq=5000, check_freq=1000)\n",
    "\n",
    "    train_start_time = time.time()\n",
    "    total_steps = ddpg_timesteps\n",
    "    steps_per_stage = 5000\n",
    "    stages = total_steps // steps_per_stage\n",
    "    print(f\"开始分阶段训练DDPG，总步数: {total_steps}，分为{stages}个阶段\")\n",
    "\n",
    "    import gc\n",
    "\n",
    "    for stage in range(stages):\n",
    "        print(f\"阶段 {stage+1}/{stages}，训练步数: {steps_per_stage}\")\n",
    "        model_ddpg.learn(\n",
    "            total_timesteps=steps_per_stage,\n",
    "            callback=callbacks,\n",
    "            tb_log_name=f\"ddpg_stage_{stage}\",\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"DDPG 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "    model_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg_20150101~20250101\")\n",
    "    print(f\"最终模型已保存至 {TRAINED_MODEL_DIR}/agent_ddpg_20150101~20250101\")\n",
    "    print(f\"检查点保存在 {TRAINED_MODEL_DIR}/checkpoints/ddpg/\")\n",
    "    print(f\"最佳模型已保存至 {TRAINED_MODEL_DIR}/best/ddpg/\")\n",
    "    gc.collect()\n",
    "    print(\"内存已清理\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 模型\n",
    "if if_using_ppo:\n",
    "    print(\"\\n======== 开始训练 PPO 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    PPO_PARAMS = {\n",
    "        \"n_steps\": 512,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 256,\n",
    "        \"device\": gpu_device,\n",
    "    }\n",
    "    model_ppo = agent.get_model(\"ppo\", model_kwargs=PPO_PARAMS)\n",
    "    tmp_path = RESULTS_DIR + \"/ppo\"\n",
    "    new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_ppo.set_logger(new_logger_ppo)\n",
    "    callbacks = setup_callbacks(\"ppo\", env_train, eval_freq=5000, check_freq=1000)\n",
    "\n",
    "    train_start_time = time.time()\n",
    "    total_steps = ppo_timesteps\n",
    "    steps_per_stage = 5000\n",
    "    stages = total_steps // steps_per_stage\n",
    "    print(f\"开始分阶段训练PPO，总步数: {total_steps}，分为{stages}个阶段\")\n",
    "\n",
    "    import gc\n",
    "\n",
    "    try:\n",
    "        for stage in range(stages):\n",
    "            print(f\"阶段 {stage+1}/{stages}，训练步数: {steps_per_stage}\")\n",
    "            model_ppo.learn(\n",
    "                total_timesteps=steps_per_stage,\n",
    "                callback=callbacks,\n",
    "                tb_log_name=f\"ppo_stage_{stage}\",\n",
    "            )\n",
    "            gc.collect()\n",
    "            checkpoint_path = f\"{TRAINED_MODEL_DIR}/agent_ppo_checkpoint_{stage+1}\"\n",
    "            model_ppo.save(checkpoint_path)\n",
    "            print(f\"保存阶段性检查点: {checkpoint_path}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n训练被用户中断，保存当前模型...\")\n",
    "        interrupt_path = f\"{TRAINED_MODEL_DIR}/agent_ppo_interrupted\"\n",
    "        model_ppo.save(interrupt_path)\n",
    "        print(f\"中断模型已保存至: {interrupt_path}\")\n",
    "\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"PPO 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "    model_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo_20150101~20250101\")\n",
    "    print(f\"最终模型已保存至 {TRAINED_MODEL_DIR}/agent_ppo_20150101~20250101\")\n",
    "    print(f\"检查点保存在 {TRAINED_MODEL_DIR}/checkpoints/ppo/\")\n",
    "    print(f\"最佳模型已保存至 {TRAINED_MODEL_DIR}/best/ppo/\")\n",
    "    gc.collect()\n",
    "    print(\"内存已清理\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD3 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD3 模型\n",
    "if if_using_td3:\n",
    "    print(\"\\n======== 开始训练 TD3 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    TD3_PARAMS = {\n",
    "        \"batch_size\": 256,\n",
    "        \"buffer_size\": 300000,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"device\": gpu_device,\n",
    "    }\n",
    "    model_td3 = agent.get_model(\"td3\", model_kwargs=TD3_PARAMS)\n",
    "    tmp_path = RESULTS_DIR + \"/td3\"\n",
    "    new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_td3.set_logger(new_logger_td3)\n",
    "    callbacks = setup_callbacks(\"td3\", env_train, eval_freq=5000, check_freq=1000)\n",
    "\n",
    "    train_start_time = time.time()\n",
    "    print(f\"开始训练，总步数: {td3_timesteps}\")\n",
    "    trained_td3 = model_td3.learn(\n",
    "        total_timesteps=td3_timesteps, callback=callbacks, tb_log_name=\"td3\"\n",
    "    )\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"TD3 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "    trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3_20150101~20250101\")\n",
    "    print(f\"最终模型已保存至 {TRAINED_MODEL_DIR}/agent_td3_20150101~20250101\")\n",
    "    print(f\"检查点保存在 {TRAINED_MODEL_DIR}/checkpoints/td3/\")\n",
    "    print(f\"最佳模型已保存至 {TRAINED_MODEL_DIR}/best/td3/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC 模型\n",
    "if if_using_sac:\n",
    "    print(\"\\n======== 开始训练 SAC 模型 ========\")\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    SAC_PARAMS = {\n",
    "        \"batch_size\": 256,\n",
    "        \"buffer_size\": 300000,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"learning_starts\": 100,\n",
    "        \"ent_coef\": \"auto_0.1\",\n",
    "        \"device\": gpu_device,\n",
    "    }\n",
    "    model_sac = agent.get_model(\"sac\", model_kwargs=SAC_PARAMS)\n",
    "    tmp_path = RESULTS_DIR + \"/sac\"\n",
    "    new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    model_sac.set_logger(new_logger_sac)\n",
    "    callbacks = setup_callbacks(\"sac\", env_train, eval_freq=5000, check_freq=1000)\n",
    "\n",
    "    train_start_time = time.time()\n",
    "    print(f\"开始训练，总步数: {sac_timesteps}\")\n",
    "    trained_sac = model_sac.learn(\n",
    "        total_timesteps=sac_timesteps, callback=callbacks, tb_log_name=\"sac\"\n",
    "    )\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    print(f\"SAC 训练完成，耗时: {train_time:.2f}秒 ({train_time/60:.2f}分钟)\")\n",
    "    trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac_20150101~20250101\")\n",
    "    print(f\"最终模型已保存至 {TRAINED_MODEL_DIR}/agent_sac_20150101~20250101\")\n",
    "    print(f\"检查点保存在 {TRAINED_MODEL_DIR}/checkpoints/sac/\")\n",
    "    print(f\"最佳模型已保存至 {TRAINED_MODEL_DIR}/best/sac/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练过程\n",
    "def visualize_training_results(model_name):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # 训练奖励\n",
    "    plt.subplot(2, 2, 1)\n",
    "    csv_path = os.path.join(RESULTS_DIR, model_name, \"*.monitor.csv\")\n",
    "    csv_files = glob.glob(csv_path)\n",
    "    if csv_files:\n",
    "        data = pd.read_csv(csv_files[0], skiprows=1)\n",
    "        plt.plot(data[\"r\"], label=\"单步奖励\", alpha=0.3, color='gray')\n",
    "        window_size = 100\n",
    "        if len(data) > window_size:\n",
    "            rolling_mean = data[\"r\"].rolling(window=window_size).mean()\n",
    "            plt.plot(rolling_mean, label=f\"奖励滚动平均(窗口={window_size})\", color='blue', linewidth=2)\n",
    "        steps = np.arange(0, len(data), 5000)\n",
    "        if len(steps) > 0 and len(data) > max(steps):\n",
    "            checkpoint_rewards = [data[\"r\"].iloc[step] if step < len(data) else np.nan for step in steps]\n",
    "            plt.scatter(steps, checkpoint_rewards, color='red', s=50, zorder=5, label='每5000步检查点')\n",
    "            for i in range(len(steps)-1):\n",
    "                if steps[i+1] < len(data):\n",
    "                    plt.plot(steps[i:i+2], data[\"r\"].iloc[steps[i:i+2]], 'r--', alpha=0.7, linewidth=1.5)\n",
    "        plt.title(f\"{model_name.upper()} 训练奖励\")\n",
    "        plt.xlabel(\"训练步数\")\n",
    "        plt.ylabel(\"奖励\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"未找到训练记录\", ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    \n",
    "    # 评估奖励\n",
    "    plt.subplot(2, 2, 2)\n",
    "    eval_path = os.path.join(RESULTS_DIR, \"eval\", model_name, \"evaluations.npz\")\n",
    "    if os.path.exists(eval_path):\n",
    "        data = np.load(eval_path)\n",
    "        rewards = data[\"results\"]\n",
    "        steps = data[\"timesteps\"]\n",
    "        mean_rewards = rewards.mean(axis=1)\n",
    "        std_rewards = rewards.std(axis=1)\n",
    "        plt.plot(steps, mean_rewards, label=\"平均评估奖励\", marker='o', color='green')\n",
    "        plt.fill_between(steps, mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.2, color='green')\n",
    "        if len(steps) > 1:\n",
    "            z = np.polyfit(steps, mean_rewards, 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(steps, p(steps), \"r--\", alpha=0.7, label=f\"趋势线 (斜率: {z[0]:.6f})\")\n",
    "        plt.title(f\"{model_name.upper()} 评估性能\")\n",
    "        plt.xlabel(\"训练步数\")\n",
    "        plt.ylabel(\"平均评估奖励\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"未找到评估记录\", ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    \n",
    "    # 检查点性能比较\n",
    "    plt.subplot(2, 2, 3)\n",
    "    checkpoint_files = sorted(glob.glob(os.path.join(TRAINED_MODEL_DIR, \"checkpoints\", model_name, \"*.zip\")))\n",
    "    if checkpoint_files:\n",
    "        steps = []\n",
    "        for file in checkpoint_files:\n",
    "            try:\n",
    "                step = int(file.split(\"_\")[-2])\n",
    "                steps.append(step)\n",
    "            except:\n",
    "                continue\n",
    "        if steps and os.path.exists(eval_path):\n",
    "            data = np.load(eval_path)\n",
    "            eval_steps = data[\"timesteps\"]\n",
    "            eval_rewards = data[\"results\"].mean(axis=1)\n",
    "            checkpoint_rewards = [eval_rewards[np.argmin(np.abs(eval_steps - step))] for step in steps]\n",
    "            plt.bar(range(len(steps)), checkpoint_rewards, color='skyblue', alpha=0.7)\n",
    "            plt.xticks(range(len(steps)), [f\"{s//1000}k\" for s in steps], rotation=45)\n",
    "            best_idx = np.argmax(checkpoint_rewards)\n",
    "            plt.bar(best_idx, checkpoint_rewards[best_idx], color='green', alpha=0.7)\n",
    "            plt.title(f\"{model_name.upper()} 检查点性能比较\")\n",
    "            plt.xlabel(\"训练步数\")\n",
    "            plt.ylabel(\"平均评估奖励\")\n",
    "            plt.grid(alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"未找到检查点文件\", ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    \n",
    "    # 收敛分析\n",
    "    plt.subplot(2, 2, 4)\n",
    "    csv_files = glob.glob(csv_path)\n",
    "    if csv_files:\n",
    "        data = pd.read_csv(csv_files[0], skiprows=1)\n",
    "        window_size = 100\n",
    "        if len(data) > window_size:\n",
    "            rolling_rewards = data[\"r\"].rolling(window=window_size).mean()\n",
    "            sample_steps = np.arange(0, len(data), 5000)\n",
    "            sample_steps = sample_steps[sample_steps >= window_size]\n",
    "            if len(sample_steps) > 1:\n",
    "                sample_rewards = [rolling_rewards.iloc[step] for step in sample_steps if step < len(rolling_rewards)]\n",
    "                sample_steps = sample_steps[:len(sample_rewards)]\n",
    "                reward_changes = np.diff(sample_rewards)\n",
    "                reward_changes = np.append(reward_changes, reward_changes[-1])\n",
    "                ax1 = plt.gca()\n",
    "                ax1.set_xlabel(\"训练步数\")\n",
    "                ax1.set_ylabel(\"滚动平均奖励\", color='blue')\n",
    "                ax1.plot(sample_steps, sample_rewards, 'o-', color='blue', label=\"滚动平均奖励\")\n",
    "                ax1.tick_params(axis='y', labelcolor='blue')\n",
    "                ax2 = ax1.twinx()\n",
    "                ax2.set_ylabel(\"奖励变化率\", color='red')\n",
    "                ax2.plot(sample_steps, reward_changes, 'o--', color='red', label=\"奖励变化率\")\n",
    "                ax2.tick_params(axis='y', labelcolor='red')\n",
    "                converged = np.all(np.abs(reward_changes[-3:]) < 0.01) if len(reward_changes) >= 3 else False\n",
    "                title = f\"{model_name.upper()} 收敛分析 - \"\n",
    "                title += \"已收敛\" if converged else \"训练中\"\n",
    "                plt.title(title)\n",
    "                lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "                lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "                ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "                plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(\"results/figures\", exist_ok=True)\n",
    "    plt.savefig(f\"results/figures/{model_name}_training_analysis.png\", dpi=300)\n",
    "    print(f\"可视化结果已保存至 results/figures/{model_name}_training_analysis.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结训练结果\n",
    "trained_models = []\n",
    "if if_using_a2c:\n",
    "    trained_models.append(\"A2C\")\n",
    "if if_using_ddpg:\n",
    "    trained_models.append(\"DDPG\")\n",
    "if if_using_ppo:\n",
    "    trained_models.append(\"PPO\")\n",
    "if if_using_td3:\n",
    "    trained_models.append(\"TD3\")\n",
    "if if_using_sac:\n",
    "    trained_models.append(\"SAC\")\n",
    "\n",
    "print(\"\\n======== 训练完成 ========\")\n",
    "print(f\"使用设备: {'GPU (CUDA)' if use_cuda else 'CPU'}\")\n",
    "print(f\"训练的模型: {', '.join(trained_models)}\")\n",
    "print(f\"所有模型已保存至 {TRAINED_MODEL_DIR} 目录\")\n",
    "print(\"\\n下一步: 运行 back_test.ipynb 来评估训练好的模型表现\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
